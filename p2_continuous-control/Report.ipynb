{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Nanodegree\n",
    "## Project 2 \"Continous Control\"\n",
    "\n",
    "Author: Marcelo Vilela\n",
    "\n",
    "Date: 7/Jan/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document I summarize my implementation from Continous Control project - part of Deep Reinforcement Learning Nanodegree. You can find the goal and project details [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Starting point\n",
    "I used the DDPG bipedal ddpg_agent.py and model.py as a starting point, all my implementation was based on this code as it fit well enough on this project. First I ran the code with no modification to understand how it performs and be able to compare the algorithm evolution on each step.\n",
    "\n",
    "I could not achieve good results using the vanilla implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improvements\n",
    "In this section I detailed the improvements on the implementation. There were mainly 4 enhancements on the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Network architecture\n",
    "After some experimentation in the network architecture I ended with a 2 fully connected hidden layers (128/128) architecture. Also, I decided to add a batch normalization between fc1 and fc2 layers, it improved the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Copy network weight\n",
    "The second improvement was to copy the network weights (actor and critic) on the initialization. To do that I created a function (copy_weight) and updated when the agent is initialized. This was applied for the actor and the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Gradient clipping\n",
    "The last improvement on the algorithm was the gradient clipping, I used the torch.nn.utils.clip_grad_norm_ function to do this when updating both actor and critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Hyperparameters\n",
    "I've played around with hyperparameters, mainly with learning rate (for actor and critic that I kept equal in the end) and the sigma on Ornstein-Uhlenbeck process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Results\n",
    "The final result after all improvements was satisfatory, solving the problem after 282 episodes. As you can see in the chart after ~ 100 episodes the score trends improve signifcantly and after ~ 200 episodes it become flatten and achieve the average score from the last 100 episodes on episode 282."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Further work\n",
    "I belive the network architecture may cause a big impact on the solution, and I would like to experiment more on this, adding more layers and normalization. Also, changing the noise in the network can help the agent to learn and I think this is an area that may bring positive results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
