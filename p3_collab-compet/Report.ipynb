{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Reinforcement Learning Nanodegree\n",
    "\n",
    "## Project 3: Collaboration and Competition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "For this project, you will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting Started\n",
    "\n",
    "1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n",
    "    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n",
    "    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n",
    "    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n",
    "    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n",
    "    \n",
    "    (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n",
    "\n",
    "    (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the \"headless\" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen, but you will be able to train the agent.  (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above._)\n",
    "\n",
    "2. Place the file in the DRLND GitHub repository, in the `p3_collab-compet/` folder, and unzip (or decompress) the file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instructions\n",
    "\n",
    "Follow the instructions in `Tennis.ipynb` to get started with training your own agent!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Goal\n",
    "\n",
    "The main objective of this project is to train two agents to play tennis. The goal for each agent is to keep the ball in play. The agent receives a reward of +0.1 for each action it takes that keep the ball in game, otherwise it receives a reward of -0.01.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Solution approach\n",
    "\n",
    "To solve the environment I chosed the following apporach:\n",
    "1. Select the algorithm that fits well in the problem\n",
    "2. Test different parameters to find the best configuration set\n",
    "\n",
    "PS: Below you will find a table with some tests, I have done more tests that I listed but I decided to remove the ones that made no impact on the final result. The optimization was build to solve the enviroment faster. Also, I ran each combination set only once and restarted the kernel for each new execution, bear in mind this you may have different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Selecting the algorithm\n",
    "To select the an algorithm there were environmental and high-level architecture considerations.\n",
    "\n",
    "The action space consists in 2 possible continuous actions. For simplicitys sake, I decided to use a policy-based algorithm - as it will much more suitable for this kind of problems.\n",
    "\n",
    "Also, we have two agents on this environment, for that reason I chosed the Multi-agent DDPG to solve this problem. This algorithm will also take advantage from stochastic policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Actor-Critic\n",
    "Actor-Critic was implemented from the beginning, as it makes the algorithm more stable and faster.\n",
    "\n",
    "Using a policy-based approach, the actor learns how to act by directly estimating the optimal policy and maximizing reward. Meanwhile, employing a value-based approach, the critic learns how to estimate the value of different state-action pairs. Actor-critic methods combine these two approaches in order to accelerate the learning process.\n",
    "\n",
    "I used local and target networks to improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Replay Buffer\n",
    "The replay buffer contains a collection of experience with the state, action, reward, and next state. The critic samples from this buffer as part of the learning step. Experiences are sampled randomly, so that the data is uncorrelated. This prevents action values from oscillating or diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Ornstein-Uhlenbeck process\n",
    "To solve the exploration-exploitation dilema, as we are in a continous action space, a good solution is to implement Ornstein-Uhlenbeck process. This will add noise to each action value, allowing the agent to explore the more action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Network architecture\n",
    "The neural network achitecture for Actor and Critic are the same, both with a hidden layer (256x128) and a batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Results\n",
    "\n",
    "I tabulated the results and parameters used to achieve it. I saved the model of the best result so it will be easy to load and test it.\n",
    "\n",
    "|#|Buffer size|Batch size|LR Actor|LR Critic|Weight Decay|Learn every|Learn num|Gamma|Tau|Eps start|Eps Ep End|Eps final|OU Sigma|OU Theta|Batch normalization?|Episodes to solve|\n",
    "|-|--------|---------|--------|---------|------------|-----------|---------|-----|---|---------|-----|-|----|---|-|---|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|1|0.99|7e-2|5.0|300|0|0.2|0.15|No|>2000|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|5|0.99|7e-2|5.0|300|0|0.2|0.15|No|1151|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|5|0.99|7e-2|5.0|300|0|0.2|0.15|Yes|744|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|5|0.99|7e-2|5.0|300|0|0.2|0.12|Yes|636|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|5|0.99|7e-2|5.0|250|0|0.2|0.12|Yes|872|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|5|5|0.99|7e-2|5.0|300|0|0.2|0.12|Yes|1031|\n",
    "|1|int(1e6)|128|1e-3|1e-3|0|1|5|0.99|7e-2|5.5|300|0|0.2|0.12|Yes|748|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the score plot\n",
    "![Scores](plot.jpg \"Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Future work\n",
    "One possible way to improve the algorithm would be prioritize the experience replay rather than always sampling experiences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
